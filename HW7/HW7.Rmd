---
title: "HW7"
author: "Alexandra Gibbons"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(tidybayes)
library(rstantools)
library(rethinking)
library(tidybayes.rethinking)
library(splines)
library(ggdag)
library(dagitty)
```

## 6E1
Multicollinearity: This occurs when we add two or more predictor variables that are strongly associated, conditional on other variables. Having both predictor variables can make it seem like the predictors are not strongly associated with the outcome even if they are in reality. This occurs because once you know the value of one of the two (or more) strongly associated variables you do not get much additional information about the predicted outcome by adding the other predictor. 

Post-treatment bias: This occurs when we condition on things downstream of treatment that are mediators, biasing inference of the effect of the treatment and potentially masking it. 

Collider bias: Conditioning on a collider (a common consequence of two causes) can make two non-associated variables (ex: A and B) seem associated because knowing the value of one of them (ex: A), plus the value of the collider, gives the model information about the value of the other (ex: B),  regardless of this is an unobserved cause, thus biasing inference.

## 6E2
Example of post-treatment bias: If evaluating the effectiveness of postsecondary prison education programs, E, at reducing recidivism rates, R, (let's say a 1 year recidivism rate), conditioning on whether someone has a job, J, 6 months after their release would induce post-treatment bias because this is a consequence of the prison education program (the treatment) and it could impact whether or not  someone recidivates.

```{r}
dag_coords <- tibble(
  name=c("E", "J", "R"),
  x=c(1, 1.5, 1.5),
  y=c(2, 2, 1))

dagify(J ~ E,
       R ~ J,
       R ~ E,
       coords=dag_coords) %>% 
  ggplot(aes(x=x, y=y, xend=xend, yend=yend)) +
  geom_dag_point(color="deepskyblue3", alpha=1/4, size=11) +
  geom_dag_text(color="deepskyblue3") +
  geom_dag_edges(edge_color="steelblue4") +
  scale_x_continuous(NULL, breaks = NULL, expand = c(0.1, 0.1)) +
  scale_y_continuous(NULL, breaks = NULL, expand = c(0.2, 0.2)) +
  theme(panel.grid = element_blank())
```



## 6E3 
I am assuming that the book is talking about conditional independencies.

The Fork: $$X \leftarrow Z \rightarrow Y$$
```{r}
dag_fork <- dagitty("dag {
  X <- Z -> Y
}")

impliedConditionalIndependencies(dag_fork)
```

Once you condition on Z, X and Y are independent. 

The Pipe: $$X \rightarrow Z \rightarrow Y$$
```{r}
dag_pipe <- dagitty("dag {
                    X -> Z -> Y
                    }")

impliedConditionalIndependencies(dag_pipe)
```

Once you condition on Z, X and Y are independent. 

The Collider: $$X \rightarrow Z \leftarrow Y$$
```{r}
dag_collider <- dagitty("dag {
                        X -> Z <- Y
                        }")

impliedConditionalIndependencies(dag_collider)
```

X is independent of Y. However, if you condition on Z, X and Y are no longer independent. 

The last elemental confound is the descendant. An example is below. Also, just realized that we can use the pipe operator for this.
```{r}
dagitty("dag{
        X <- Z -> Y
        Z -> D}") %>% 
  impliedConditionalIndependencies()
```

If D is a descendant of Z, then conditioning on D means that we are partly conditioning on Z. As seen above, conditioning on Z makes D independent of both X and Y. 

## 6E4 
For this example: $$A \rightarrow B \leftarrow C$$
A biased sample is like conditioning on a collider, B, because conditioning on a collider will make the golem find a spurious association between A and C (ex: in the marriage example the model looks for associations between happiness and age among married sample and unmarried sample separately instead of looking across the whole sample b/c is conditioned on marriage status). This is similar to what happens what happens in a biased sample that is selected based on its features (ex: based on A and C) that then then end up in the statistical model and are associated in the sample even though they are not across the whole population.  

## 6M1
```{r}
dag_coords <- tibble(
  name = c("U", "X", "A", "B", "C", "Y", "V"),
  x = c(1, 1, 1.5, 1.5, 2, 2, 2.3),
  y = c(2, 1, 2.3, 1.7, 2, 1, 1.5))

dagify(X ~ U,
       U ~ A, 
       B ~ U + C,
       C ~ A + V, 
       Y ~ X + C + V, 
       coords = dag_coords) %>% 
  ggplot(aes(x=x, y=y, xend=xend, yend=yend)) +
  geom_dag_point(color="deepskyblue3", alpha=1/4, size=11) +
  geom_dag_text(color="deepskyblue3") +
  geom_dag_edges(edge_color="steelblue4") +
  scale_x_continuous(NULL, breaks = NULL, expand = c(0.1, 0.1)) +
  scale_y_continuous(NULL, breaks = NULL, expand = c(0.2, 0.2)) +
  theme(panel.grid = element_blank())
  
```

There are five paths that connect X to Y: 

This path is all good because it is a frontdoor path:
$$X \rightarrow Y$$ 

This path is closed because B is a collider:
$$X \leftarrow U \rightarrow B \leftarrow C \rightarrow Y$$ 
 
This path is closed because B is a collider:
$$X \leftarrow U \rightarrow B \leftarrow C \leftarrow V \rightarrow Y$$ 

This path is open but can be closed by conditioning on A, a fork:
$$X \leftarrow U \leftarrow A \rightarrow C \rightarrow Y$$ 

This path is open but can be closed by conditioning on A, a fork:
$$X \leftarrow U \leftarrow A \rightarrow C \rightarrow Y$$ 

There are only two paths that we need to close, and both can be closed by conditioning on A. So, we should condition on A to estimate the causal effect of X on Y. 

## 6M2
```{r}
m2 <- tibble(X=rnorm(1e4, 0, 1),
             Z=rnorm(1e4, X, .01),
             Y=rnorm(1e4, Z, .3))

modelm2 <- quap(alist(
  Y ~ dnorm(mu, sigma),
  mu <- a + bX*X + bZ*Z,
  a ~ dnorm(0, .5),
  bX ~ dnorm(0, 3),
  bZ ~ dnorm(0, 3),
  sigma ~ exp(1)),
  data=m2)

precis(modelm2)
```

No, this is not an example of multicollinearity as there isn't an underlying unobserved thing causing both X and Z here. This DAG is a pipe so conditioning on Z blocks the path from X to Y and makes X and Y independent of one another, which is why we don't observe a strong association between X and Y. This is different from the legs example because X causes Z, which is a mediating variable, and then Z causes Y, whereas in the legs example both leg lengths are caused by the same thing, genes.

## 6M3 
Top left: Two open backdoor paths (sorry Nico, too lazy and sleepy to write them out here but I did sketch out all of the paths on papers), both of which can be closed by conditioning on Z.

Top right: No backdoor paths, so no need to condition on anything. 

Bottom left: One backdoor path but it is closed as Z is a collider, so again no need to condition on anything. 

Bottom right: One open backdoor path, we should condition on A to block this path; conditioning on Z is not an option because that would block the pipe from X to Y via Z. 
